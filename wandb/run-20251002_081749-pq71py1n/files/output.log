Train 数据集: 找到 3106 对有效的图像-标签对
使用波段: RGB (3通道)
Val 数据集: 找到 1034 对有效的图像-标签对
使用波段: RGB (3通道)
Test 数据集: 找到 1035 对有效的图像-标签对
使用波段: RGB (3通道)
正在加载预训练权重: ./pretrained_weights/vit_base_patch16_224.pth
使用融合策略: interpolate
预训练权重包含 152 个键
检测到TransCC模型有 6 层transformer blocks
检测到预训练模型有 12 层transformer blocks
✓ 基础组件匹配: patch_embed.proj.weight -> encoder.patch_embed.proj.weight
✓ 基础组件匹配: patch_embed.proj.bias -> encoder.patch_embed.proj.bias
✓ 基础组件匹配: cls_token -> encoder.cls_token
✓ 基础组件匹配: norm.weight -> encoder.norm.weight
✓ 基础组件匹配: norm.bias -> encoder.norm.bias
使用线性插值策略：将12层插值为6层
✓ 插值映射: layer0.0 -> layer0
✓ 插值映射: layer2.2 -> layer1
✓ 插值映射: layer4.4 -> layer2
✓ 插值映射: layer6.6 -> layer3
✓ 插值映射: layer8.8 -> layer4
✓ 插值映射: layer11.0 -> layer5
✓ 成功加载 77 个预训练权重
✓ 预训练权重加载完成
2025-10-02 08:17:52,471 INFO: 实验开始: TransCC_V2_1002_0816
2025-10-02 08:17:52,472 INFO: 模型: TransCC_V2, 总参数量: 57,756,842, 可训练参数量: 57,756,842
Training Epoch 1:   0%|                                                                       | 0/258 [00:00<?, ?it/s]
2025-10-02 08:17:53,032 ERROR: An error occurred: CUDA out of memory. Tried to allocate 578.00 MiB. GPU 0 has a total capacity of 31.35 GiB of which 195.50 MiB is free. Process 990097 has 29.19 GiB memory in use. Including non-PyTorch memory, this process has 1.78 GiB memory in use. Of the allocated memory 1.17 GiB is allocated by PyTorch, and 24.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/rove/BuindingsSeg/train/train_transccV2.py", line 231, in main
    train_total, train_seg, train_bd, train_result = train_one_epoch(
  File "/home/rove/BuindingsSeg/train/train_transccV2.py", line 49, in train_one_epoch
    outputs = model(images)
  File "/home/rove/BuindingsSeg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1777, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rove/BuindingsSeg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1883, in _call_impl
    return inner()
  File "/home/rove/BuindingsSeg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1831, in inner
    result = forward_call(*args, **kwargs)
  File "/home/rove/BuindingsSeg/models/TransCCV2.py", line 499, in forward
    encoder_features, layer_features = self.encoder(x)
  File "/home/rove/BuindingsSeg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1777, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rove/BuindingsSeg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1788, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rove/BuindingsSeg/models/TransCCV2.py", line 279, in forward
    x = block(x)
  File "/home/rove/BuindingsSeg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1777, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rove/BuindingsSeg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1788, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rove/BuindingsSeg/models/TransCCV2.py", line 170, in forward
    x = x + self.drop_path(self.attn(self.norm1(x)))
  File "/home/rove/BuindingsSeg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1777, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rove/BuindingsSeg/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1788, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rove/BuindingsSeg/models/TransCCV2.py", line 28, in forward
    attn = (q @ k.transpose(-2, -1)) * self.scale
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 578.00 MiB. GPU 0 has a total capacity of 31.35 GiB of which 195.50 MiB is free. Process 990097 has 29.19 GiB memory in use. Including non-PyTorch memory, this process has 1.78 GiB memory in use. Of the allocated memory 1.17 GiB is allocated by PyTorch, and 24.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
